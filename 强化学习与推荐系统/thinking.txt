Thinking1	机器学习中的监督学习、非监督学习、强化学习有何区别			简要说出三者之间的区别（10points）

强化学习的思路和人比较类似，是在实践中学习
比如学习走路，如果摔倒了，那么我们大脑后面会给一个负面的奖励值 => 这个走路姿势不好；如果后面正常走了一步，那么大脑会给一个正面的奖励值 => 这是一个好的走路姿势
与监督学习的区别，没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的（比如走路摔倒）
与非监督学习的区别，在非监督学习中既没有输出值也没有奖励值的，只有数据特征，而强化学习有奖励值（为负是为惩罚），此外非舰队学习与监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系

Thinking2	什么是策略网络，价值网络，有何区别			简要说明两者之间的区别（10points）

策略网络:给定特定的输入，通过学习给出一个确定输出的网络。
价值网络:通过计算目前状态s的累积分数的期望，给游戏中的状态赋予一个数值/分数。每个状态都经历了整个数值网络。奖赏更多的状态在数值网络中的值更大。

价值网络和策略网络结构相似，都有多层神经网络，但是里面的系数不一样：策略网络通过梯度计算公式进行更新，而价值网络根据目标值进行更新

Thinking3	请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的			简要说明4个步骤的原理（10points）

蒙特卡洛树搜索是一个搜索算法，结合了随机模拟的一般性和树搜索的准确性，它采用的各种方法都是为了有效地减少搜索空间。
在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树
MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域

使用主要步骤：
1.选择，从根节点开始，按一定策略，搜索到叶子节点
2.扩展，对叶子节点扩展一个或多个合法的子节点
3.模拟，对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数
4.回传，根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点


Thinking4：	假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑			简要说明强化学习在信息流推荐中的应用，有自己的观点，（10points）

抖音中的广告推荐若使用强化学习，需要考虑几个因素：1.是否插入广告；2.插入什么广告；3.在什么位置插入广告。
最终以用户的 广告观看时长为/总广告时长 的百分比为判断依据：满分100分，得分越高，说明该广告越成功

Thinking5：	在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路			简要说明强化学习在自动驾驶中的应用，有自己的观点，（10points）

自动驾驶中的强化学习，输入是各种环境信息，比如车道线的类型、距离，前方/侧面车辆的速度、距离，行人的速度、距离，交通信号灯的状态、距离等
输出包括三个因素：1.方向盘的状态；2.油门的状态；3.刹车的状态