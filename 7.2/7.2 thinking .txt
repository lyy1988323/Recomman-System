Thinking1	参数共享指的是什么？
神经网络在每个位置进行特征提取的时候都是共享一个卷积核，大大减少参数量
Thinking2	为什么会用到batch normalization ?
BN就是通过一定的规范化手段，把每层神经网络任意神经元　这个输入值的分布强行拉回到均值为0方差为1的标准正态分布
使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失问题产生），同时也让收敛速度更快，加快训练速度
Thinking3	使用dropout可以解决什么问题？
可以随机删掉神经元，避免过拟合问题